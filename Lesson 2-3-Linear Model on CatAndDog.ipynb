{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear models in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irashadow/.pyenv/versions/miniconda-3.9.1/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:541: UserWarning: Theano flag device=gpu* (old gpu back-end) only support floatX=float32. You have floatX=float64. Use the new gpu back-end with device=cuda* for that value of floatX.\n",
      "  warnings.warn(msg)\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "from theano.sandbox import cuda\n",
    "theano.sandbox.cuda.use(\"gpu0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import utils; reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = random((30,2))\n",
    "y = np.dot(x, [2., 3.]) + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4991,  0.8699],\n",
       "       [ 0.2255,  0.3724],\n",
       "       [ 0.2187,  0.861 ],\n",
       "       [ 0.7387,  0.1383],\n",
       "       [ 0.229 ,  0.5722]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.6081,  2.5682,  4.0204,  2.8923,  3.1744])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm = Sequential([ Dense(1, input_shape=(2,))])\n",
    "lm.compile(optimizer=SGD(lr=0.1), loss = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.832939147949219"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.evaluate(x, y, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.3162],\n",
       "        [-0.734 ]], dtype=float32), array([ 0.], dtype=float32)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "30/30 [==============================] - 0s - loss: 0.0096     \n",
      "Epoch 2/5\n",
      "30/30 [==============================] - 0s - loss: 0.0046     \n",
      "Epoch 3/5\n",
      "30/30 [==============================] - 0s - loss: 0.0021     \n",
      "Epoch 4/5\n",
      "30/30 [==============================] - 0s - loss: 9.3706e-04 \n",
      "Epoch 5/5\n",
      "30/30 [==============================] - 0s - loss: 4.4664e-04     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8d458b7710>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(x, y, nb_epoch=5, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00022208085283637047"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.evaluate(x, y, verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train linear model on predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We now know enough to understand how to modify Vgg16 to create a model that will output predictions for Cats and Dogs. Follow along in the lesson 2 notebook to be able to reproduce what we're going to do.\n",
    "\n",
    "# Adding a Dense Layer\n",
    "The dense layer we used in the last section mapped input vectors to a single output. We can easily change this to output to a vector of arbitrary length, noting that the structure of the weights for such an output will be just a matrix like we talked about earlier.\n",
    "\n",
    "The last layer of Vgg16 outputs a vector of 1000 categories, because that is the number of categories the competition asked for. Of these categories, some of them certainly correspond to cats and dogs, but at a much more granular level (specific breeds). We could manually figure out which of these categories are cats and which are dogs, and simply write some code that will translate the imagenet classifications into a cat and dog classification. But that would be inefficient, and we would miss some key information.\n",
    "\n",
    "A better approach would be to simply add a Dense layer on top of the imagenet layer, and train the model to map the imagenet classifications of input images of cats and dogs to cat and dog labels. Why is this better than manually doing it? Because the neural network will utilize all information available to it from the imagenet classifications, as opposed to simply mapping cat categories to cat and dog categories to dog. For example, a picture of a german sheperd with a bone would likely have strong probabilities in the german sheperd category and the bone category. If we only map the dog categories to dog, and throw out the rest, then we lose this other information useful in classification, such as whether or not a bone is in the image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irashadow/.pyenv/versions/miniconda-3.9.1/lib/python2.7/site-packages/keras/layers/core.py:577: UserWarning: `output_shape` argument not specified for layer lambda_4 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 3, 224, 224)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n"
     ]
    }
   ],
   "source": [
    "from vgg16 import Vgg16\n",
    "vgg = Vgg16()\n",
    "model = vgg.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our overall approach here will be:  \n",
    "\n",
    "1. Get the true labels for every image  \n",
    "2. Get the 1,000 imagenet category predictions for every image  \n",
    "3. Feed these predictions as input to a simple linear model. \n",
    "\n",
    "The lesson 2 notebook shows us again how to grab our batches. One important point to note is that the labels we get from our batches need to be one-hot encoded. One hot encoding simply takes categorical variables, and converts them into a matrix where each column represents a category. If an image belongs to category A, that image's row in the matrix has a 1 in the category A column and 0's every else. One important reason we take the step to convert our labels to these vectors is because this is the same shape as the output of our dense layer. Therefore for training purposes we need to one-hot encode them. Now we can predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 23000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "path = '/home/irashadow/python_workspace/deep_learning_workspace/data/DogAndCat/'\n",
    "val_batches = get_batches(path+'valid',shuffle=False, batch_size=1)\n",
    "batches = get_batches(path+'train',shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bcolz\n",
    "def save_array(fname, arr): c=bcolz.carray(arr, rootdir=fname, mode='w'); c.flush()\n",
    "def load_array(fname): return bcolz.open(fname)[:]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??get_data_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_data = get_data_batches(val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trn_data = get_data_batches(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path+'models/train_data.bc', trn_data)\n",
    "save_array(path+'models/valid_data.bc', val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_data = load_array(path+'models/train_data.bc')\n",
    "val_data = load_array(path+'models/valid_data.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 3, 224, 224)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras returns classes as a single column, so we convert to one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def onehot(x): return np.array(OneHotEncoder().fit_transform(x.reshape(-1,1)).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_classes = val_batches.classes\n",
    "trn_classes = batches.classes\n",
    "val_labels = onehot(val_classes)\n",
    "trn_labels = onehot(trn_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23000, 2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_classes[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_labels[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trn_features = model.predict(trn_data, batch_size=batch_size)\n",
    "val_features = model.predict(val_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_array(path+'models/train_lastlayer_features.bc', trn_features)\n",
    "save_array(path+'models/valid_lastlayer_features.bc', val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_features = load_array(path+'models/train_lastlayer_features.bc')\n",
    "val_features = load_array(path+'models/valid_lastlayer_features.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23000, 1000)\n"
     ]
    }
   ],
   "source": [
    "print trn_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1000 inputs, since that's saved features, and 2 outputs, for dog and cat\n",
    "lm = Sequential([ Dense(2, activation='softmax', input_shape=(1000,)) ])\n",
    "lm.compile(optimizer=RMSprop(lr=0.1), loss = 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "23000/23000 [==============================] - 0s - loss: 0.0991 - acc: 0.9656 - val_loss: 0.0781 - val_acc: 0.9750\n",
      "Epoch 2/3\n",
      "23000/23000 [==============================] - 0s - loss: 0.0829 - acc: 0.9737 - val_loss: 0.0767 - val_acc: 0.9770\n",
      "Epoch 3/3\n",
      "23000/23000 [==============================] - 0s - loss: 0.0833 - acc: 0.9761 - val_loss: 0.0828 - val_acc: 0.9770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8d3351ddd0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(trn_features, trn_labels, nb_epoch=3, batch_size=batch_size, validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_6 (Dense)                  (None, 2)             2002        dense_input_3[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 2,002\n",
      "Trainable params: 2,002\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you remember how we defined our linear model? Here it is again for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm = Sequential([ Dense(2, activation='softmax', input_shape=(1000,)) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And do you remember the definition of a fully connected layer in the original VGG?:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(4096, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning\n",
    "If we observe the last layer of Vgg16, we can see that the last layer is simply a dense layer that outputs 1000 elements, which is as we'd expect. Therefore, it seems somewhat unreasonable to stack a dense layer meant to find cats and dogs on top of one that's meant to find imagenet categories, in that we're limiting the information available to us by first coercing the neural network to classify to imagenet before cats and dogs.\n",
    "\n",
    "Instead, let's remove that last layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_4 (Lambda)                (None, 3, 224, 224)   0           lambda_input_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_40 (ZeroPadding2D) (None, 3, 226, 226)   0           lambda_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_40 (Convolution2D) (None, 64, 224, 224)  1792        zeropadding2d_40[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_41 (ZeroPadding2D) (None, 64, 226, 226)  0           convolution2d_40[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_41 (Convolution2D) (None, 64, 224, 224)  36928       zeropadding2d_41[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_16 (MaxPooling2D)   (None, 64, 112, 112)  0           convolution2d_41[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_42 (ZeroPadding2D) (None, 64, 114, 114)  0           maxpooling2d_16[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_42 (Convolution2D) (None, 128, 112, 112) 73856       zeropadding2d_42[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_43 (ZeroPadding2D) (None, 128, 114, 114) 0           convolution2d_42[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_43 (Convolution2D) (None, 128, 112, 112) 147584      zeropadding2d_43[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_17 (MaxPooling2D)   (None, 128, 56, 56)   0           convolution2d_43[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_44 (ZeroPadding2D) (None, 128, 58, 58)   0           maxpooling2d_17[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_44 (Convolution2D) (None, 256, 56, 56)   295168      zeropadding2d_44[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_45 (ZeroPadding2D) (None, 256, 58, 58)   0           convolution2d_44[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_45 (Convolution2D) (None, 256, 56, 56)   590080      zeropadding2d_45[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_46 (ZeroPadding2D) (None, 256, 58, 58)   0           convolution2d_45[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_46 (Convolution2D) (None, 256, 56, 56)   590080      zeropadding2d_46[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_18 (MaxPooling2D)   (None, 256, 28, 28)   0           convolution2d_46[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_47 (ZeroPadding2D) (None, 256, 30, 30)   0           maxpooling2d_18[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_47 (Convolution2D) (None, 512, 28, 28)   1180160     zeropadding2d_47[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_48 (ZeroPadding2D) (None, 512, 30, 30)   0           convolution2d_47[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_48 (Convolution2D) (None, 512, 28, 28)   2359808     zeropadding2d_48[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_49 (ZeroPadding2D) (None, 512, 30, 30)   0           convolution2d_48[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_49 (Convolution2D) (None, 512, 28, 28)   2359808     zeropadding2d_49[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_19 (MaxPooling2D)   (None, 512, 14, 14)   0           convolution2d_49[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_50 (ZeroPadding2D) (None, 512, 16, 16)   0           maxpooling2d_19[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_50 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_50[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_51 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_50[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_51 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_51[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_52 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_51[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_52 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_52[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_20 (MaxPooling2D)   (None, 512, 7, 7)     0           convolution2d_52[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 25088)         0           maxpooling2d_20[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_24 (Dense)                 (None, 4096)          102764544   flatten_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 4096)          0           dense_24[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_25 (Dense)                 (None, 4096)          16781312    dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 4096)          0           dense_25[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_31 (Dense)                 (None, 2)             8194        dropout_8[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 134,268,738\n",
      "Trainable params: 8,194\n",
      "Non-trainable params: 134,260,544\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove last layer\n",
    "model.pop()\n",
    "for layer in model.layers: layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen=image.ImageDataGenerator()\n",
    "batches = gen.flow(trn_data, trn_labels, batch_size=batch_size, shuffle=True)\n",
    "val_batches = gen.flow(val_data, val_labels, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "??vgg.finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opt = RMSprop(lr=0.1)\n",
    "model.compile(optimizer=opt, loss = 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_model(model, batches, val_batches, nb_epoch=1):\n",
    "    model.fit_generator(batches, samples_per_epoch=batches.N, nb_epoch=nb_epoch, \n",
    "                        validation_data=val_batches, nb_val_samples=val_batches.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 3968/23000 [====>.........................] - ETA: 456s - loss: 0.8280 - acc: 0.9413"
     ]
    }
   ],
   "source": [
    "fit_model(model,batches,val_batches,nb_epoch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'models/finetune1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(path+'models/finetune1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 48s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.080610379279838523, 0.97799999999999998]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_data, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 48s    \n",
      "2000/2000 [==============================] - 48s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.    ,  1.    ,  0.5166,  1.    ,  1.    ,  1.    ,  1.    ,  1.    ], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict_classes(val_data, batch_size=batch_size)\n",
    "probs = model.predict_proba(val_data, batch_size=batch_size)[:,0]\n",
    "probs[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(val_classes, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1030   17]\n",
      " [  27  926]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU8AAAEpCAYAAAATYKC1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWwMxHbDbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8d2d19a950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(cm, {'cat':0, 'dog':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
